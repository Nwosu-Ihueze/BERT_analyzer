{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca6rYUxNi_MO"
   },
   "source": [
    "# Stage 1: Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5-6CWGy5IBT0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pho8wNAcIom5",
    "outputId": "170730c6-ac1a-4587-bc88-5edfff72c16e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/d3/820ccaf55f1e24b5dd43583ac0da6d86c2d27bbdfffadbba69bafe73ca93/bert-for-tf2-0.14.7.tar.gz (41kB)\n",
      "\r",
      "\u001b[K     |████████                        | 10kB 20.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 20kB 26.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 30kB 28.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 40kB 31.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n",
      "\u001b[?25hCollecting py-params>=0.9.6\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
      "Collecting params-flow>=0.8.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
      "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.7-cp36-none-any.whl size=30537 sha256=38f6c223e04863755fa97f4c43c1ddcfa20455f47bdef88c3a23c03f18d41fec\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/f8/e2/b98f79a6b8cc898d8e4102b83acb8a098df7d27500a2bac912\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=ac0d5b5a1918c551423aed6cc0f6a526bfb5f0df8a6bb5d1ce68b3d2e3673689\n",
      "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19475 sha256=10be7df365c47b5d90e599972a345f8664da186ba4d504c0216fd1071d189c10\n",
      "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
      "Successfully built bert-for-tf2 py-params params-flow\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.14.7 params-flow-0.8.2 py-params-0.9.7\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 12.8MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.94\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fOh80JlMJVAr"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0_xu0I3jFP9"
   },
   "source": [
    "# Stage 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v60JTFKojIq5"
   },
   "source": [
    "## Loading files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTtO7NkPjKUd"
   },
   "source": [
    "We import files from our personal Google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mqoi9IvJcpm",
    "outputId": "ca976569-e6bd-4248-e831-a1916c6aebda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iJU3EXxXKDU_"
   },
   "outputs": [],
   "source": [
    "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "data = pd.read_csv(\n",
    "    \"/content/drive/MyDrive/BERT_analyzer/training.1600000.processed.noemoticon.csv\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_fp5LoS-KRB6"
   },
   "outputs": [],
   "source": [
    "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
    "          axis=1,\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Quzx5tnjUtl"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8hlexmRjXIS"
   },
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YUYxqVYOKaAy"
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
    "    \n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "    \n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "    \n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
    "    \n",
    "    tweet = re.sub(r\" +\", ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UNsMIV1hKk-8"
   },
   "outputs": [],
   "source": [
    "data_clean = [clean_tweet(tweet) for tweet in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ifMqzeEFKp94"
   },
   "outputs": [],
   "source": [
    "data_labels = data.sentiment.values\n",
    "data_labels[data_labels == 4] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eh7sIquja5t"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K59PriX4jgBV"
   },
   "source": [
    "We need to create a BERT layer to have access to meta data for the tokenizer (like vocab size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qMT3msV4LjP6"
   },
   "outputs": [],
   "source": [
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\",\n",
    "                            trainable=False)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AMZtPwFfLnLD"
   },
   "outputs": [],
   "source": [
    "def encode_sentence(sent):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MUyx7WAJLqfU"
   },
   "outputs": [],
   "source": [
    "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-4oGSu5jxUi"
   },
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLg0Z7QOj_YZ"
   },
   "source": [
    "We will create padded batches (so we pad sentences for each batch independently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length, apply padded_batches and then shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-6Gzgx19L0A7"
   },
   "outputs": [],
   "source": [
    "data_with_len = [[sent, data_labels[i], len(sent)]\n",
    "                 for i, sent in enumerate(data_inputs)]\n",
    "random.shuffle(data_with_len)\n",
    "data_with_len.sort(key=lambda x: x[2])\n",
    "sorted_all = [(sent_lab[0], sent_lab[1])\n",
    "              for sent_lab in data_with_len if sent_lab[2] > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1ihqOkr-L3Gh"
   },
   "outputs": [],
   "source": [
    "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
    "                                             output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXmQ0ua4L6tw",
    "outputId": "34f8627e-dd6e-4afb-b35a-a70fa7c809c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=int32, numpy=array([4122, 1037, 2204, 3185, 2000, 2272, 2006, 2694], dtype=int32)>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(all_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JDDbHGcuRzch"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "kmmyyQ2TR3dd"
   },
   "outputs": [],
   "source": [
    "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
    "NB_BATCHES_TEST = NB_BATCHES // 10\n",
    "all_batched.shuffle(NB_BATCHES)\n",
    "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
    "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxONsFVHkFLU"
   },
   "source": [
    "# Stage 3: Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "onjAiMjlMC6f"
   },
   "outputs": [],
   "source": [
    "class DCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 emb_dim=128,\n",
    "                 nb_filters=50,\n",
    "                 FFN_units=512,\n",
    "                 nb_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"dcnn\"):\n",
    "        super(DCNN, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size,\n",
    "                                          emb_dim)\n",
    "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
    "                                    kernel_size=2,\n",
    "                                    padding=\"valid\",\n",
    "                                    activation=\"relu\")\n",
    "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
    "                                     kernel_size=3,\n",
    "                                     padding=\"valid\",\n",
    "                                     activation=\"relu\")\n",
    "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
    "                                      kernel_size=4,\n",
    "                                      padding=\"valid\",\n",
    "                                      activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=nb_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs)\n",
    "        x_1 = self.bigram(x) # batch_size, nb_filters, seq_len-1)\n",
    "        x_1 = self.pool(x_1) # (batch_size, nb_filters)\n",
    "        x_2 = self.trigram(x) # batch_size, nb_filters, seq_len-2)\n",
    "        x_2 = self.pool(x_2) # (batch_size, nb_filters)\n",
    "        x_3 = self.fourgram(x) # batch_size, nb_filters, seq_len-3)\n",
    "        x_3 = self.pool(x_3) # (batch_size, nb_filters)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSix1l4jkIxp"
   },
   "source": [
    "# Stage 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "7vwWBjRtMGu6"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "61ZAFKEVMI77"
   },
   "outputs": [],
   "source": [
    "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
    "            emb_dim=EMB_DIM,\n",
    "            nb_filters=NB_FILTERS,\n",
    "            FFN_units=FFN_UNITS,\n",
    "            nb_classes=NB_CLASSES,\n",
    "            dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "yHlUVRVbML7G"
   },
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "else:\n",
    "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "CVKErIhpMO1p"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./drive/My Drive/BERT_analyzer/ckpt_bert_tok/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VXF0b_-iMRtZ"
   },
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ckpt_manager.save()\n",
    "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drsbr4hQMUZE",
    "outputId": "5d25985c-bf17-4765-ea34-004dff4e9ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  37196/Unknown - 2116s 57ms/step - loss: 0.4296 - accuracy: 0.8021Checkpoint saved at ./drive/My Drive/BERT_analyzer/ckpt_bert_tok/.\n",
      "37196/37196 [==============================] - 2117s 57ms/step - loss: 0.4296 - accuracy: 0.8021\n",
      "Epoch 2/5\n",
      "37195/37196 [============================>.] - ETA: 0s - loss: 0.3817 - accuracy: 0.8300Checkpoint saved at ./drive/My Drive/BERT_analyzer/ckpt_bert_tok/.\n",
      "37196/37196 [==============================] - 2056s 55ms/step - loss: 0.3817 - accuracy: 0.8300\n",
      "Epoch 3/5\n",
      "37196/37196 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.8512Checkpoint saved at ./drive/My Drive/BERT_analyzer/ckpt_bert_tok/.\n",
      "37196/37196 [==============================] - 2065s 56ms/step - loss: 0.3420 - accuracy: 0.8512\n",
      "Epoch 4/5\n",
      "37196/37196 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.8712Checkpoint saved at ./drive/My Drive/BERT_analyzer/ckpt_bert_tok/.\n",
      "37196/37196 [==============================] - 2060s 55ms/step - loss: 0.3019 - accuracy: 0.8712\n",
      "Epoch 5/5\n",
      "37196/37196 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.8883Checkpoint saved at ./drive/My Drive/BERT_analyzer/ckpt_bert_tok/.\n",
      "37196/37196 [==============================] - 2056s 55ms/step - loss: 0.2641 - accuracy: 0.8883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0c759ffb70>"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dcnn.fit(train_dataset,\n",
    "         epochs=NB_EPOCHS,\n",
    "         callbacks=[MyCustomCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IiDW919kQQK"
   },
   "source": [
    "# Stage 5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tIHkXynpSfm7",
    "outputId": "3bea5070-21ef-4357-a38f-200f53ed494e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4132/4132 [==============================] - 23s 5ms/step - loss: 0.4461 - accuracy: 0.8309\n",
      "[0.44614285230636597, 0.8308627605438232]\n"
     ]
    }
   ],
   "source": [
    "results = Dcnn.evaluate(test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "15a1u5RbSgOU"
   },
   "outputs": [],
   "source": [
    "def get_prediction(sentence):\n",
    "    tokens = encode_sentence(sentence)\n",
    "    inputs = tf.expand_dims(tokens, 0)\n",
    "\n",
    "    output = Dcnn(inputs, training=False)\n",
    "\n",
    "    sentiment = math.floor(output*2)\n",
    "\n",
    "    if sentiment == 0:\n",
    "        print(\"Ouput of the model: {}\\nPredicted sentiment: negative.\".format(\n",
    "            output))\n",
    "    elif sentiment == 1:\n",
    "        print(\"Ouput of the model: {}\\nPredicted sentiment: positive.\".format(\n",
    "            output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qdKnaf0sSkrF",
    "outputId": "de0b3545-f47e-4cc6-c951-e9d269588ae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ouput of the model: [[0.95234746]]\n",
      "Predicted sentiment: positive.\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"A really interesting outcome\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "B-4oGSu5jxUi",
    "VxONsFVHkFLU",
    "vSix1l4jkIxp"
   ],
   "name": "Copy of Bert_tokenizer",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
